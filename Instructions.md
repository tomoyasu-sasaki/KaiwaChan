# キャラクター表示システム 実装ステップ指示書

## 概要
この指示書では、既存の下記パイプライン

1. **音声認識 (Whisper)**
2. **対話生成 (LLaMA 2)**
3. **音声合成 (VOICEVOX)**

に加えて、**2Dキャラクター表示システム**を段階的に実装する手順を示します。最終的に、**自然な表情やモーション、リップシンクを伴う2Dキャラクター**が音声入力→対話→音声出力のフローと連動することを目標とします。

---

## ステップ0: 環境準備
1. **Python環境の整備**  
   - Python 3.9 以上を推奨  
   - 仮想環境（venv など）を準備

2. **必要ライブラリのインストール**  
   - 例: `pip install pygame` / `pip install kivy` / `pip install pycocos2d` など、選定したフレームワークに応じてインストール  
   - Whisper, LLaMA 2, VOICEVOX が動く環境をすでに用意  
   - 音声再生用ライブラリ（`pyaudio` など）やGUI用途のライブラリ（`pyqt5` など）も必要に応じてインストール

3. **プロジェクト構成ディレクトリの作成**  
```
my_project/ ├── main.py ├── modules/ │ ├── speech_recognition.py (Whisper関連) │ ├── dialogue_generation.py (LLaMA 2関連) │ ├── speech_synthesis.py (VOICEVOX関連) │ └── character_animation.py (キャラクター表示システム 追加予定) └── assets/ ├── images/ │ └── ... (キャラの表情やスプライトなど) └── sounds/ └── ... (BGMやSEなど, 必要に応じて)
```


---

## ステップ1: キャラクター表示の最小実装
1. **`character_animation.py` に基本描画ロジックを記述**  
- 2D描画フレームワークの初期化 (Pygame / Kivy / cocos2d など)  
- ウィンドウを開いてキャラの立ち絵（静止画）を表示する最小コードを実装  
- メインループで画面を更新し続ける仕組みを整備

2. **表情や口パクの差分画像を用意**  
- 例: `neutral.png`, `mouth_open.png`, `mouth_closed.png` など  
- まだリップシンクは行わず、キー入力でテスト的に口パクを切り替えられればOK

3. **メインプログラム `main.py` に簡易接続**  
- `character_animation.py` のテスト呼び出し  
- 例: `python main.py` 実行時にキャラが表示されるか確認

---

## ステップ2: Whisper→LLaMA→VOICEVOX の連動とキャラ表示の統合
1. **既存パイプラインとの簡易連携**  
- `speech_recognition.py` からテキスト取得  
- `dialogue_generation.py` で応答を生成  
- `speech_synthesis.py` で音声出力（VOICEVOX）  
- この一連の流れを `main.py` で呼び出し、**ターミナル表示や単純な音声再生**で確認

2. **キャラクター表示システムにイベントフック**  
- 音声合成の開始・終了イベントを捕捉し、合成中だけ口パクアニメを再生（今回は疑似的に口を開閉するだけ）  
- GUIスレッドとAI処理スレッドを分ける (スレッドや非同期イベントループなど)

3. **感情表現の初期実装**  
- LLaMA 2 に「返答の感情タグ」を簡易的に返すプロンプトを仕込む (例: `{"emotion": "happy", "text": "メッセージ"}` の形式)  
- 取得した感情に応じて、固定で表情を変える (例: "happy" → ニコニコ顔, "sad" → 泣き顔)


## ステップ3: リップシンクの実装 (テキストベース)
1. **テキストからの音素解析**  
- VOICEVOX が利用している形態素解析や Open JTalk などを活用して、入力テキストから音素列を取得  
- 簡易化として、母音「a/i/u/e/o」ごとに口形状を割り当てる

2. **タイムテーブルの生成**  
- 単語ごと or 音素ごとに、おおまかな再生時間を割り振る (200msごとに次の音素など、目安値でOK)  
- 音声合成の開始と同時にタイムテーブルに沿って口形状を切り替える

3. **アニメーションの実装**  
- メインループで現在時刻 (音声再生からの経過時間) とタイムテーブルを比較し、対応する口形状に切り替え  
- 最初は多少ズレても良しとする

---

## ステップ4: リップシンクの改善 (オーディオベース / 正確な同期)
1. **VOICEVOX から得られる音素タイミング情報の活用**  
- VOICEVOX の音声合成APIでタイムスタンプ付きの音素情報が取得できる場合は、それをもとに正確に口形状を変化  
- タイムスタンプを `[(0.00s, "あ"), (0.10s, "い"), ...]` のような配列で受け取り、同様の手順でアニメを同期

2. **再生位置のリアルタイム取得**  
- 音声を再生するライブラリ側で、現在の再生位置(ミリ秒単位)を取得  
- その時刻に最も近い音素を参照して口形状を更新

3. **イベント駆動 / コールバック方式**  
- 音声再生中にコールバックイベントを登録し、音素切り替えタイミングで呼び出す  
- Python向けのライブラリではサポート状況が異なるので、必要に応じて工夫

---

## ステップ5: 表情遷移と感情表現の強化
1. **感情スコアリングの細分化**  
- 「喜び度: 0.8」「驚き度: 0.3」など、複数感情を重みづけして返す仕組みにし、表情をブレンド  
- Live2DモデルやSpineなどの高度なアセットを使う場合は、リグのパラメータに合わせて重み付け可能

2. **自然な補間アニメ**  
- 表情A→Bへ移行する際に、数フレームかけてフェードする  
- 目パチ（瞬き）や顔の揺れなどをアイドリングで入れる

3. **複数キャラ対応**  
- モジュールを拡張して異なるキャラクターごとにボイスとアニメを切り替え  
- キャラクタ別の発話速度や表情を設定

---

## ステップ6: エラー処理・例外管理・最適化
1. **通信の途切れ・モジュールの失敗時**  
- Whisper / LLaMA / VOICEVOX いずれかが落ちた時のリカバリ  
- タイムアウト設定など

2. **パフォーマンス計測**  
- フレームレート維持（PygameやKivyの描画が遅くならないか）  
- AI推論部がボトルネックにならないよう、非同期処理を最適化

3. **スケーラビリティ**  
- 対話ログの保存や、マルチスレッド対応  
- 大規模モデルに差し替える場合や、3Dモデル化の将来拡張の検討

---

## ステップ7: テストとデモ
1. **ユニットテスト / 結合テスト**  
- 音声認識→対話生成→音声合成→キャラ描画までの一貫テスト  
- 感情タグが正しく反映されているか、口パクがズレていないかをチェック

2. **デモ用シナリオ作成**  
- 特定のセリフを喋らせたり、複数感情を含む対話を試す  
- 動画キャプチャやスクリーンショットで成果を記録

3. **リリース / 公開**  
- ソースコード整理、READMEドキュメント整備  
- キャラクター素材のライセンス確認

---

## まとめ
1. **ステップを小さく分割し、まずは静止画→基本アニメ→リップシンク→感情表現の順に拡張**  
2. **テキストベースのリップシンクで簡易実装し、必要に応じて音声ベースへ高度化**  
3. **感情推定やアニメーション補間などは徐々に強化**し、最初から複雑にしすぎない  
4. **モジュール分割と非同期設計が重要**。表示系とAI推論系を疎結合に保つ  
