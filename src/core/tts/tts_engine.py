import requests
import json
import soundfile as sf
import numpy as np
from pathlib import Path
import time
import logging
import tempfile
import os
import uuid
import torch
from typing import Optional, Dict, Union, Any
import sounddevice as sd
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
from rubyinserter import add_ruby
import warnings


class TTSEngine:
    """
    ãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°åˆæˆï¼ˆTTSï¼‰ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
    VOICEVOXã¨Japanese Parler-TTSã‚’ã‚µãƒãƒ¼ãƒˆã—ã€è¨­å®šã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    """

    def __init__(self, config=None):
        """ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿"""
        self.logger = logging.getLogger(__name__)
        
        # è­¦å‘Šã¨ãƒ­ã‚°ã®æŠ‘åˆ¶
        self._suppress_warnings()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.base_url = "http://localhost:50021"
        self.speaker_id = 1
        self.max_retries = 3
        self.retry_delay = 1.0
        self.timeout = 30
        self.sample_rate = 24000
        self.cache_size = 100
        self.cache_enabled = True
        self.current_engine = "voicevox"
        self.description = "A female speaker with a slightly high-pitched voice delivers her words at a moderate speed with a quite monotone tone in a confined environment, resulting in a quite clear audio recording"

        # è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
        self.load_config(config)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
        self.cache_dir = Path(tempfile.gettempdir()) / "kaiwachan" / "tts_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache = {}

        try:
            # Parler-TTSã®åˆæœŸåŒ–ï¼ˆParler-TTSã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
            if self.current_engine == "parler":
                self.initialize_parler_tts()
            self.logger.info("TTSã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"TTSã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise

    def _suppress_warnings(self):
        """è­¦å‘Šã¨ãƒ­ã‚°ã®æŠ‘åˆ¶"""
        # Parler-TTSã®è­¦å‘Šã‚’æŠ‘åˆ¶
        logging.getLogger("parler_tts.modeling_parler_tts").setLevel(logging.ERROR)
        warnings.filterwarnings("ignore", message=".*weight_norm is deprecated.*")
        
        # transformersã®è­¦å‘Šã‚’æŠ‘åˆ¶
        logging.getLogger("transformers").setLevel(logging.ERROR)
        warnings.filterwarnings("ignore", category=FutureWarning)
        warnings.filterwarnings("ignore", category=UserWarning)
        
        # PyTorchã®è­¦å‘Šã‚’æŠ‘åˆ¶
        logging.getLogger("torch").setLevel(logging.ERROR)
        
        # Metalé–¢é€£ã®è­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆbf16é–¢é€£ï¼‰
        os.environ["METAL_DEBUG_ERROR_MODE"] = "0"
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

    def load_config(self, config):
        """è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
        if config is None:
            return
        
        try:
            models_config = config.get("models")
            self.current_engine = models_config.get("current_engine", self.current_engine)

            # VOICEVOXè¨­å®š
            voicevox_config = models_config.get("voicevox")
            if voicevox_config:
                self.base_url = voicevox_config.get("url", self.base_url)
                self.speaker_id = voicevox_config.get("speaker_id", self.speaker_id)
                self.max_retries = voicevox_config.get("max_retries", self.max_retries)
                self.retry_delay = voicevox_config.get("retry_delay", self.retry_delay)
                self.timeout = voicevox_config.get("timeout", self.timeout)
                self.cache_size = voicevox_config.get("cache_size", self.cache_size)
                self.cache_enabled = voicevox_config.get("cache_enabled", self.cache_enabled)

            # Parler-TTSè¨­å®š
            parler_config = models_config.get("parler")
            if parler_config:
                self.description = parler_config.get("description", self.description)
                self.model_path = parler_config.get("model_path", "2121-8/japanese-parler-tts-mini-bate")
                self.use_fast_tokenizer = parler_config.get("use_fast_tokenizer", True)
                self.add_prefix_space = parler_config.get("add_prefix_space", False)

        except Exception as e:
            self.logger.error(f"è¨­å®šã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    def initialize_parler_tts(self):
        """Parler-TTSãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–"""
        try:
            self.logger.info("Parler-TTSãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
            self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
            
            # ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–è¨­å®š
            model_kwargs = {
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,  # åŠç²¾åº¦ã§å®Ÿè¡Œ
            }

            model = ParlerTTSForConditionalGeneration.from_pretrained(
                self.model_path,
                **model_kwargs
            ).to(self.device)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.eval()
            
            # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if hasattr(torch, 'compile'):
                try:
                    self.model = torch.compile(model)
                    self.logger.info("ãƒ¢ãƒ‡ãƒ«ã‚’JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§æœ€é©åŒ–ã—ã¾ã—ãŸ")
                except Exception as e:
                    self.logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    self.model = model
            else:
                self.model = model
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ï¼ˆé«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                use_fast=self.use_fast_tokenizer,
                add_prefix_space=self.add_prefix_space,
                model_max_length=512  # æœ€å¤§é•·ã‚’åˆ¶é™ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã‚’æœ€é©åŒ–
            )
            
            self.logger.info("âœ… Parler-TTSãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ Parler-TTSãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            raise

    def synthesize(self, text: str, speaker_id: Optional[int] = None) -> Optional[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã‚’åˆæˆã™ã‚‹

        Args:
            text: åˆæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            speaker_id: è©±è€…IDï¼ˆVOICEVOXã®å ´åˆã®ã¿ä½¿ç”¨ï¼‰
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if not text or text.isspace():
            self.logger.warning("ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸã€‚éŸ³å£°åˆæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return None

        try:
            
            return (
                self.synthesize_with_parler(text)
                if self.current_engine == "parler"
                else self.synthesize_with_voicevox(text, speaker_id)
            )
        except Exception as e:
            self.logger.error(f"éŸ³å£°åˆæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None

    def synthesize_with_parler(self, text: str) -> Optional[str]:
        """Parler-TTSã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’åˆæˆ"""
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if self.cache_enabled:
                cache_key = f"parler_{text}_{self.description}"
                if cache_key in self.cache:
                    self.logger.info("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éŸ³å£°ã‚’å–å¾—")
                    return self.cache[cache_key]

            self.logger.info(f"ğŸ”Š Parler-TTSã§éŸ³å£°åˆæˆã‚’é–‹å§‹: {text[:30]}...")

            # ãƒ«ãƒ“å‡¦ç†
            text = add_ruby(text)
            
            with torch.inference_mode():  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ï¼‰
                # èª¬æ˜æ–‡ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
                description_tokens = self.tokenizer(
                    self.description,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).input_ids.to(self.device)
                
                # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
                text_tokens = self.tokenizer(
                    text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).input_ids.to(self.device)
                
                # éŸ³å£°ç”Ÿæˆ
                generation = self.model.generate(
                    input_ids=description_tokens,
                    prompt_input_ids=text_tokens
                )
                
                # CPUä¸Šã§numpyé…åˆ—ã«å¤‰æ›
                audio_arr = generation.cpu().numpy().squeeze()

            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            audio_path = self.cache_dir / f"tts_parler_{uuid.uuid4().hex}.wav"
            sf.write(str(audio_path), audio_arr, self.model.config.sampling_rate)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if self.cache_enabled:
                self.cache[cache_key] = str(audio_path)
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®åˆ¶é™
                if len(self.cache) > self.cache_size:
                    oldest_key = next(iter(self.cache))
                    oldest_file = Path(self.cache[oldest_key])
                    if oldest_file.exists():
                        oldest_file.unlink()
                    del self.cache[oldest_key]

            self.logger.info(f"âœ… Parler-TTSéŸ³å£°åˆæˆå®Œäº†: {audio_path}")
            return str(audio_path)

        except Exception as e:
            self.logger.error(f"âŒ Parler-TTSéŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def synthesize_with_voicevox(self, text: str, speaker_id: Optional[int] = None) -> Optional[str]:
        """VOICEVOXã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’åˆæˆ"""
        current_speaker = speaker_id or self.speaker_id

        self.logger.info(f"ğŸ”Š VOICEVOXéŸ³å£°åˆæˆã‚’é–‹å§‹: {text[:30]}...")

        try:
            params = {"text": text, "speaker": current_speaker}
            query = requests.post(f"{self.base_url}/audio_query", params=params, timeout=self.timeout)
            query.raise_for_status()

            query_data = query.json()
            synthesis = requests.post(
                f"{self.base_url}/synthesis",
                headers={"Content-Type": "application/json"},
                params={"speaker": current_speaker},
                data=json.dumps(query_data),
                timeout=self.timeout,
            )
            synthesis.raise_for_status()

            audio_path = self.cache_dir / f"tts_voicevox_{uuid.uuid4().hex}.wav"
            with open(audio_path, "wb") as f:
                f.write(synthesis.content)

            self.logger.info(f"âœ… VOICEVOXéŸ³å£°åˆæˆå®Œäº†: {audio_path}")
            return str(audio_path)

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ VOICEVOXéŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_system_info(self) -> Dict[str, Any]:
        """
        ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—ã™ã‚‹
        
        Returns:
            ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¾æ›¸
        """
        info = {
            'current_engine': self.current_engine,
            'cache_enabled': self.cache_enabled,
            'cache_size': self.cache_size,
        }
        
        if self.current_engine == "voicevox":
            info.update({
                'voicevox_url': self.base_url,
                'speaker_id': self.speaker_id,
            })
        elif self.current_engine == "parler":
            info.update({
                'device': getattr(self, 'device', 'not_initialized'),
                'model_loaded': hasattr(self, 'model'),
            })
            
        return info
