from llama_cpp import Llama
from pathlib import Path
import logging
import time
import json
from typing import List, Dict, Optional, Any

class DialogueEngine:
    """
    å¯¾è©±ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
    
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
    LLMã‚’ä½¿ç”¨ã—ã¦è‡ªç„¶ãªä¼šè©±å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, config=None):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.logger = logging.getLogger(__name__)
        self.config = config
        self.model = None
        self.conversation_history = []
        self.max_history = 10
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if config:
            self._initialize_model()
    
    def _initialize_model(self) -> bool:
        """
        LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹
        
        Returns:
            bool: åˆæœŸåŒ–ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            model_path = self.config.get_model_path("llama")
            if not model_path.exists():
                self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                raise FileNotFoundError(
                    f"Graniteãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}\n"
                    "1. https://huggingface.co/lmstudio-community/granite-3.1-8b-instruct-GGUF ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
                    f"2. {model_path}ã«é…ç½®ã—ã¦ãã ã•ã„"
                )
            
            self.logger.info(f"LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
            start_time = time.time()
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
            model_config = self.config.get_app_config("models", "llm", {})
            n_threads = model_config.get("n_threads", 8)
            n_batch = model_config.get("n_batch", 512)
            n_ctx = model_config.get("n_ctx", 2048)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.model = Llama(
                model_path=str(model_path),
                n_ctx=n_ctx,
                n_gpu_layers=-1,  # åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®GPUãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨
                n_threads=n_threads,
                n_batch=n_batch,
                seed=42
            )
            
            load_time = time.time() - start_time
            self.logger.info(f"LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ (æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’)")
            return True
            
        except Exception as e:
            self.logger.error(f"LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    def generate_response(self, user_input: str, character_info: Optional[Dict] = None) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            character_info: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
            
            # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
            if self.model is None:
                if not self._initialize_model():
                    return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è©±ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ
            system_prompt = self._generate_system_prompt(character_info)
            
            self.logger.info("ğŸ¤– å¿œç­”ã‚’ç”Ÿæˆä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
            llm_config = self.config.get_app_config("models", "llm", {})
            max_tokens = llm_config.get("max_tokens", 128)
            
            # ä¼šè©±å±¥æ­´ã®æº–å‚™
            messages = [{"role": "system", "content": system_prompt}]
            
            # éå»ã®ä¼šè©±ã‚’è¿½åŠ 
            for h in self.conversation_history:
                messages.append({"role": h["role"], "content": h["content"]})
            
            # æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
            messages.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "user", "content": user_input})
            
            # å¿œç­”ã®ç”Ÿæˆ
            response = self.model.create_chat_completion(
                messages,
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                repeat_penalty=1.1,
                stop=["<|end_of_text|>", "<|start_of_role|>"]
            )
            
            # å¿œç­”ã®å¾Œå‡¦ç†
            assistant_response = response["choices"][0]["message"]["content"]
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({"role": "assistant", "content": assistant_response})
            
            # ä¼šè©±å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
            if len(self.conversation_history) > self.max_history * 2:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥å¤–ã®å¤ã„ä¼šè©±ã‚’å‰Šé™¤
                self.conversation_history = self.conversation_history[-self.max_history * 2:]
            
            return assistant_response
            
        except Exception as e:
            self.logger.error(f"å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _generate_system_prompt(self, character_info: Optional[Dict] = None) -> str:
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            character_info: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
        character_name = "AI"
        character_personality = "è¦ªã—ã¿ã‚„ã™ãä¸å¯§"
        character_speaking_style = "ç°¡æ½”ã§è‡ªç„¶ãªæ—¥æœ¬èª"
        
        if character_info:
            character_name = character_info.get("name", character_name)
            character_personality = character_info.get("personality", character_personality)
            character_speaking_style = character_info.get("speaking_style", character_speaking_style)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = f"""ã‚ãªãŸã¯{character_name}ã¨ã„ã†åå‰ã®{character_personality}ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
{character_speaking_style}ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
è³ªå•ã«å¯¾ã—ã¦ç°¡æ½”ã«å›ç­”ã—ã€å¿…è¦ä»¥ä¸Šã®èª¬æ˜ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"""
        
        return system_prompt
    
    def _clean_response(self, text: str) -> str:
        """
        ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹
        
        Args:
            text: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ä¸è¦ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
        prefixes = ["Assistant:", "AI:", "å›ç­”:", "\n"]
        result = text
        
        for prefix in prefixes:
            if result.startswith(prefix):
                result = result[len(prefix):].strip()
        
        # è³ªå•ã‚„å›ç­”ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†å‰²ã—ã€æœ€åˆã®å¿œç­”ã®ã¿ã‚’å–å¾—
        cut_patterns = [
            "\nãƒ¦ãƒ¼ã‚¶ãƒ¼:", "\nè³ªå•:", "\nå›ç­”:",
            "User:", "Assistant:",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼:", "AI:",
            "è³ªå•:", "å›ç­”:"
        ]
        
        for pattern in cut_patterns:
            if pattern in result:
                result = result.split(pattern)[0].strip()
        
        return result
    
    def _update_history(self, user_input: str, assistant_response: str) -> None:
        """
        ä¼šè©±å±¥æ­´ã‚’æ›´æ–°ã™ã‚‹
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            assistant_response: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
        """
        # æ–°ã—ã„å¯¾è©±ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        self.conversation_history.append({
            "user": user_input,
            "assistant": assistant_response,
            "timestamp": time.time()
        })
        
        # å±¥æ­´ã®é•·ã•ã‚’åˆ¶é™
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def clear_history(self) -> None:
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
        self.conversation_history = []
        self.logger.info("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        
    def set_max_history(self, max_history: int) -> None:
        """
        ä¿æŒã™ã‚‹ä¼šè©±å±¥æ­´ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’è¨­å®šã™ã‚‹
        
        Args:
            max_history: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°
        """
        if max_history < 1:
            max_history = 1
        self.max_history = max_history
        
        # ç¾åœ¨ã®å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def save_conversation(self, file_path: str) -> bool:
        """
        ä¼šè©±å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
        
        Args:
            file_path: ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: ä¿å­˜ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            if not self.conversation_history:
                self.logger.warning("ä¿å­˜ã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
                
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"ä¼šè©±å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")
            return False
    
    def load_conversation(self, file_path: str) -> bool:
        """
        ä¼šè©±å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
        
        Args:
            file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                history = json.load(f)
                
            if isinstance(history, list):
                self.conversation_history = history
                
                # å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                if len(self.conversation_history) > self.max_history:
                    self.conversation_history = self.conversation_history[-self.max_history:]
                    
                self.logger.info(f"ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(self.conversation_history)}ã‚¿ãƒ¼ãƒ³): {file_path}")
                return True
            else:
                self.logger.error("ç„¡åŠ¹ãªä¼šè©±å±¥æ­´å½¢å¼ã§ã™")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¼šè©±å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return False 