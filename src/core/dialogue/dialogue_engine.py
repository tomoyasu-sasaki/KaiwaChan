from llama_cpp import Llama
from pathlib import Path
import logging
import time
import json
from typing import List, Dict, Optional, Any
from src.config.settings_manager import SettingsManager
import os
import warnings

class DialogueEngine:
    """
    å¯¾è©±ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
    
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
    LLMã‚’ä½¿ç”¨ã—ã¦è‡ªç„¶ãªä¼šè©±å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, settings_manager: SettingsManager):
        """
        åˆæœŸåŒ–
        
        Args:
            settings_manager: è¨­å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        self.settings_manager = settings_manager
        self.logger = logging.getLogger(__name__)
        
        # è­¦å‘Šã®æŠ‘åˆ¶
        self._suppress_warnings()
        
        self.model = None
        self.conversation_history = []
        self.max_history = 10
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        model_path = self.settings_manager.get_model_path("llm")
        if not model_path.exists():
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

        # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å–å¾—
        model_config = self.settings_manager.get_app_config("models", "llm", {})
        self.n_threads = model_config.get("n_threads", 8)
        self.n_batch = model_config.get("n_batch", 512)
        self.max_tokens = model_config.get("max_tokens", 128)
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self._initialize_model()
    
    def _suppress_warnings(self):
        """è­¦å‘Šã¨ãƒ­ã‚°ã®æŠ‘åˆ¶"""
        # LLaMAã®è­¦å‘Šã‚’æŠ‘åˆ¶
        logging.getLogger("llama_cpp").setLevel(logging.ERROR)
        warnings.filterwarnings("ignore", category=UserWarning)
        
        # Metalé–¢é€£ã®è­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆbf16é–¢é€£ï¼‰
        os.environ["METAL_DEBUG_ERROR_MODE"] = "0"
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
    
    def _initialize_model(self) -> bool:
        """
        LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹
        
        Returns:
            bool: åˆæœŸåŒ–ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            model_path = self.settings_manager.get_model_path("llm")
            self.logger.info(f"LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
            start_time = time.time()
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.model = Llama(
                model_path=str(model_path),
                n_ctx=2048,
                n_gpu_layers=-1,  # åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®GPUãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨
                n_threads=self.n_threads,
                n_batch=self.n_batch,
                seed=42,
                verbose=False  # ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶
            )
            
            load_time = time.time() - start_time
            self.logger.info(f"LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ (æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’)")
            return True
            
        except Exception as e:
            self.logger.error(f"LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    def generate_response(self, user_input: str, character_info: Optional[Dict] = None) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            character_info: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            
            self.logger.info(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
            
            # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
            if self.model is None:
                if not self._initialize_model():
                    return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è©±ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
            # ä¼šè©±å±¥æ­´ãŒç©ºã®å ´åˆã®ã¿ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            messages = []
            if not self.conversation_history:
                system_prompt = self._generate_system_prompt(character_info)
                messages.append({"role": "system", "content": system_prompt})
                self.conversation_history.append({"role": "system", "content": system_prompt})
            else:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å±¥æ­´ã‹ã‚‰å–å¾—
                system_message = next((m for m in self.conversation_history if m["role"] == "system"), None)
                if system_message:
                    messages.append(system_message)
            
            self.logger.info("ğŸ¤– å¿œç­”ã‚’ç”Ÿæˆä¸­...")
            
            # éå»ã®ä¼šè©±ã‚’è¿½åŠ ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥å¤–ï¼‰
            for h in self.conversation_history:
                if h["role"] != "system":
                    messages.append(h)
            
            # æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
            messages.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "user", "content": user_input})
            
            # å¿œç­”ã®ç”Ÿæˆ
            response = self.model.create_chat_completion(
                messages,
                max_tokens=self.max_tokens,
                temperature=0.7,
                top_p=0.9,
                repeat_penalty=1.1,
                stop=["<|end_of_text|>", "<|start_of_role|>"]
            )
            
            # å¿œç­”ã®å¾Œå‡¦ç†
            assistant_response = response["choices"][0]["message"]["content"]
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({"role": "assistant", "content": assistant_response})
            
            # ä¼šè©±å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¿æŒï¼‰
            if len(self.conversation_history) > self.max_history * 2 + 1:  # +1 ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨
                system_message = next(m for m in self.conversation_history if m["role"] == "system")
                self.conversation_history = [system_message] + self.conversation_history[-(self.max_history * 2):]
            
            return assistant_response
            
        except Exception as e:
            self.logger.error(f"å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _generate_system_prompt(self, character_info: Optional[Dict] = None) -> str:
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            character_info: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚’config.ymlã‹ã‚‰å–å¾—
        default_character = self.settings_manager.get_app_config("behavior", "character", {})
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ã‚’å„ªå…ˆï¼‰
        character_name = default_character.get("name", "ä¼šè©±ã¡ã‚ƒã‚“")
        character_personality = default_character.get("personality", "ãŠå¬¢æ§˜")
        character_speaking_style = default_character.get("speaking_style", "ä¸å¯§ã§å„ªé›…ãªæ—¥æœ¬èª")
        
        # character_infoã§ä¸Šæ›¸ã
        if character_info:
            character_name = character_info.get("name", character_name)
            character_personality = character_info.get("personality", character_personality)
            character_speaking_style = character_info.get("speaking_style", character_speaking_style)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ
        system_prompt = f"""ã‚ãªãŸã¯{character_name}ã¨ã„ã†åå‰ã®{character_personality}ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
{character_speaking_style}ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
ç°¡æ½”ã«å›ç­”ã—ã€å¯¾è©±ãŒç¶™ç¶šã™ã‚‹ã‚ˆã†ã«æ„è­˜ã—ã¦ãã ã•ã„ã€‚å¿…è¦ä»¥ä¸Šã®èª¬æ˜ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®åˆ¶ç´„ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. å¸¸ã«æ—¥æœ¬èªã§å¿œç­”ã™ã‚‹ã“ã¨
2. ä¸€åº¦ã®å¿œç­”ã¯3æ–‡ä»¥å†…ã«åã‚ã‚‹ã“ã¨
3. ç›¸æ‰‹ã®ç™ºè¨€ã«å…±æ„Ÿã‚’ç¤ºã—ãªãŒã‚‰ä¼šè©±ã‚’é€²ã‚ã‚‹ã“ã¨
4. è³ªå•ã•ã‚Œã¦ã„ãªã„å ´åˆã§ã‚‚ã€ä¼šè©±ã‚’ç¶™ç¶šã•ã›ã‚‹ãŸã‚ã®è³ªå•ã‚’1ã¤å«ã‚ã‚‹ã“ã¨"""
        
        return system_prompt
    
    def _clean_response(self, text: str) -> str:
        """
        ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹
        
        Args:
            text: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ä¸è¦ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
        prefixes = ["Assistant:", "AI:", "å›ç­”:", "\n"]
        result = text
        
        for prefix in prefixes:
            if result.startswith(prefix):
                result = result[len(prefix):].strip()
        
        # è³ªå•ã‚„å›ç­”ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†å‰²ã—ã€æœ€åˆã®å¿œç­”ã®ã¿ã‚’å–å¾—
        cut_patterns = [
            "\nãƒ¦ãƒ¼ã‚¶ãƒ¼:", "\nè³ªå•:", "\nå›ç­”:",
            "User:", "Assistant:",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼:", "AI:",
            "è³ªå•:", "å›ç­”:"
        ]
        
        for pattern in cut_patterns:
            if pattern in result:
                result = result.split(pattern)[0].strip()
        
        return result
    
    def _update_history(self, user_input: str, assistant_response: str) -> None:
        """
        ä¼šè©±å±¥æ­´ã‚’æ›´æ–°ã™ã‚‹
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            assistant_response: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
        """
        # æ–°ã—ã„å¯¾è©±ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        self.conversation_history.append({"role": "user", "content": user_input})
        self.conversation_history.append({"role": "assistant", "content": assistant_response})
        
        # å±¥æ­´ã®é•·ã•ã‚’åˆ¶é™ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¿æŒï¼‰
        if len(self.conversation_history) > self.max_history * 2 + 1:
            system_message = next(m for m in self.conversation_history if m["role"] == "system")
            self.conversation_history = [system_message] + self.conversation_history[-(self.max_history * 2):]
    
    def clear_history(self) -> None:
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
        self.conversation_history = []
        self.logger.info("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        
    def set_max_history(self, max_history: int) -> None:
        """
        ä¿æŒã™ã‚‹ä¼šè©±å±¥æ­´ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’è¨­å®šã™ã‚‹
        
        Args:
            max_history: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°
        """
        if max_history < 1:
            max_history = 1
        self.max_history = max_history
        
        # ç¾åœ¨ã®å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def save_conversation(self, file_path: str) -> bool:
        """
        ä¼šè©±å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
        
        Args:
            file_path: ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: ä¿å­˜ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            if not self.conversation_history:
                self.logger.warning("ä¿å­˜ã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
                
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"ä¼šè©±å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")
            return False
    
    def load_conversation(self, file_path: str) -> bool:
        """
        ä¼šè©±å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
        
        Args:
            file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                history = json.load(f)
                
            if isinstance(history, list):
                self.conversation_history = history
                
                # å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                if len(self.conversation_history) > self.max_history:
                    self.conversation_history = self.conversation_history[-self.max_history:]
                    
                self.logger.info(f"ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(self.conversation_history)}ã‚¿ãƒ¼ãƒ³): {file_path}")
                return True
            else:
                self.logger.error("ç„¡åŠ¹ãªä¼šè©±å±¥æ­´å½¢å¼ã§ã™")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¼šè©±å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return False 