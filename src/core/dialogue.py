from llama_cpp import Llama
from pathlib import Path
import logging

class DialogueEngine:
    def __init__(self, config):
        model_path = config.get_model_path("llama")
        if not model_path.exists():
            raise FileNotFoundError(
                f"Graniteãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}\n"
                "1. https://huggingface.co/lmstudio-community/granite-3.1-8b-instruct-GGUF ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
                f"2. {model_path}ã«é…ç½®ã—ã¦ãã ã•ã„"
            )
            
        model_config = config.config["models"]["llama"]
        self.model = Llama(
            model_path=str(model_path),
            n_ctx=2048,
            n_gpu_layers=-1,
            n_threads=model_config.get("n_threads", 8),
            n_batch=model_config.get("n_batch", 512),
            seed=42
        )
        self.context = []
        self.logger = logging.getLogger(__name__)
        
    def generate_response(self, user_input):
        self.logger.info(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
        self.logger.info("ğŸ¤– å¿œç­”ã‚’ç”Ÿæˆä¸­...")
        
        prompt = self._build_prompt(user_input)
        response = self.model(
            prompt,
            max_tokens=128,
            temperature=0.7,
            top_p=0.95,
            top_k=40,
            stream=False
        )
        
        result = response["choices"][0]["text"].strip()
        # "Assistant:"ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
        result = self._clean_response(result)
        
        self.logger.info(f"âœ… ç”Ÿæˆã•ã‚ŒãŸå¿œç­”: {result}")
        return result
        
    def _build_prompt(self, user_input):
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        system_prompt = """ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ç°¡æ½”ã§è‡ªç„¶ãªæ—¥æœ¬èªã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚è³ªå•ã®å›ç­”ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
        
        # ç›´æ¥ã®å¿œç­”ã®ã¿ã‚’è¦æ±‚
        prompt = f"{system_prompt}\n\nè³ªå•: {user_input}\nå›ç­”:"
        return prompt
        
    def _clean_response(self, text):
        # ä¸è¦ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
        prefixes = ["Assistant:", "å›ç­”:", "\n"]
        result = text
        for prefix in prefixes:
            if result.startswith(prefix):
                result = result[len(prefix):].strip()
        
        # è³ªå•ã‚„å›ç­”ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†å‰²ã—ã€æœ€åˆã®å¿œç­”ã®ã¿ã‚’å–å¾—
        patterns = [
            "\nè³ªå•:", "\nå›ç­”:",
            "User:", "Assistant:",
            "è³ªå•:", "å›ç­”:"
        ]
        
        for pattern in patterns:
            if pattern in result:
                result = result.split(pattern)[0].strip()
        
        return result 